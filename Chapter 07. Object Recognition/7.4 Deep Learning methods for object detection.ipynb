{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187efeb3-98f4-4ebc-a8dd-e76ecf462c6c",
   "metadata": {},
   "source": [
    "# 7.4 Deep Learning methods for object detection\n",
    "\n",
    "In the previous notebooks we have developed different **Machine Learning** (ML) algorithms for classifying images containing objects according to high-level features (shape, color, texture, etc.). In a nutshell, these methods use a number of representatitive images to train a classifier, which is composed by a set of parameters. Once trained, the model can be used to infer the category of objects appearing in new images not seen before.\n",
    "\n",
    "As you may now, in the recent years there is an explosion of **Deep Learning** (DL) techniques that are achieving a high performance in different tasks. Indeed, DL models are a subgroup of ML with the peculiarity of having thousands, millions of parameters. This is achieved by using multiple layers, each one consisting of a given number of parameters, hence the *deep* of DL. \n",
    "\n",
    "The choice of ML or DL techniques strongly depend on the application at hand. On the one hand, if there is available a large amount of data to train the model and the target platform where the model will run is powerful enough, then DL techniques are a good option. On the other hand, when data or computational resources' constrains exist, ML methods stands out. Moreover, while ML works with explainable high-level features and processes, DL employs low-level features and are considered black boxes with an input (e.g. an image) and an ouput (e.g. a set of detected objects).\n",
    "\n",
    "As commented, untill now we have been working with ML models. Let's take a look at DL ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4769b4-8d83-44a4-9e64-6ed7bf2130e0",
   "metadata": {},
   "source": [
    "## Problem context - Object detection for mobile robots\n",
    "\n",
    "This time we will focus on mobile robots operaing in human-centered environments, for example houses. In order to provide services, a mobile robot must be able to understand its surroundings, including the elements in its workspace. For that object detection is a critital task since it permits the robot to locate objects that can be useful in its duties. Fig 1 provides some examples of objects detected in images coming from a comera mounted on a mobile robot:\n",
    "\n",
    "<center><img src=\"./images/dnn/examples_from_robot_at_home.PNG\"><br /><i>Figure 1. Images processed by a DL model with annotations about detected objects.</i></center>$\\\\[5pt]$\n",
    "\n",
    "Fusing these detections with additional information like the robot pose within the workspace and the relative pose of the camera w.r.t., the objects can be localized in said workspace. For example, Fig. 2 shows the reconstruction of two houses where the robot has detected and placed a number of objects.\n",
    "\n",
    "<center><img src=\"./images/dnn/workspace_reconstructions.jpg\"><br /><i>Fig 2. Reconstructions of real houses built by a mobile robot.</i></center>$\\\\[5pt]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7866f-03a6-454c-9ddb-61307abc2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2d485-2916-4c84-b73a-aff8288a4c3d",
   "metadata": {},
   "source": [
    "## 7.4.1 Preparing the DL model\n",
    "\n",
    "The task of training a DL model is highly time and resources consuming, typically demanding a cluster of GPUs and huge datasets. Luckily for us, there is a bunch of already trained models freely available that we can use. And what is more, our lovely OpenCV incorporates the **DNN module** (Deep Neural Network), with permits us to run **DL inference**, that is, to feed a network with an image and get the results of its processing. This is quite interesting since we don't need to install other heavy frameworks like Tensorflow, Pytorch, Caffe or Keras often used to deal with these models. Moreover, it's not necessary to have a powerful GPU to make inference since OpenCV DNN is highly optimized to run in CPU. Good news everywhere!\n",
    "\n",
    "Since our problem is object detection, we will make use of a network trained for that aim. Concretely wee need two resources:\n",
    "- a file containing the configuration of the model to be used, and\n",
    "- another file with the weights (parameters) of said network.\n",
    "\n",
    "Depending on the dataset used to train the network, the categories that it would be able to detect differ. In this notebook we are going to employ a popular network trained with data from [Micrososf COCO](https://cocodataset.org/#home). This is the most used dataset for dealing with object detection related-tasks and considers 80 different categories:\n",
    "\n",
    "<center><img src=\"./images/dnn/COCO_categories.PNG\"><br /><i>Figure 3. Object categories considered in the COCO dataset. [Source: COCO webpage]</i></center>$\\\\[5pt]$\n",
    "\n",
    "The following figure shows an example of an image from COCO, which also includes annotations about the objects that belong to one of the said categories:\n",
    "\n",
    "<center><img src=\"./images/dnn/COCO_kitchen_example.PNG\"><br /><i>Figure 4. And image of a kitchen from COCO with annotation about the regions belonging to one of the considered categories. [Source: COCO webpage]</i></center>$\\\\[5pt]$\n",
    "\n",
    "The COCO dataset has a very nice webpage with many resources, including an explorer where you can see examples like the previous one. We encorage you to take a look at it! Having choosen the network and knowing the dataset it was trained with, its time to load the needed information.\n",
    "\n",
    "Let's start by loading a file containing the names of the categories from COCO. This is useful since the network will provide the detected objects as numbers related to the indices that the categories has in this file. The following code reads this file and also builds a numpy array that assigns to each category a random color (used for results visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eabcde-221d-4fc4-b7ff-fcc74eb9b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the COCO class names\n",
    "with open('./data/object_detection_classes_coco.txt', 'r') as f:\n",
    "    class_names = f.read().split('\\n')\n",
    "    \n",
    "# get a different color array for each of the classes\n",
    "COLORS = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "type(COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823931b-7343-4e80-9fa4-19e371ca58cd",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Exploring categories</i></b></span>**\n",
    "\n",
    "Let's explore a bit the loaded information. Write the needed code to print the categories appearing from index 40 to index 60 with the format shown in the expected output. You can also try other ranges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94054d-4177-4365-9695-355f7202f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1\n",
    "for index in None:\n",
    "    print( None )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d90597-df42-40b7-a3ac-09d071551137",
   "metadata": {},
   "source": [
    "<font color='blue'>**Expected output  </font>**\n",
    "\n",
    "```\n",
    "[ 40 ]  skateboard\n",
    "[ 41 ]  surfboard\n",
    "[ 42 ]  tennis racket\n",
    "[ 43 ]  bottle\n",
    "[ 44 ]  plate\n",
    "[ 45 ]  wine glass\n",
    "[ 46 ]  cup\n",
    "[ 47 ]  fork\n",
    "[ 48 ]  knife\n",
    "[ 49 ]  spoon\n",
    "[ 50 ]  bowl\n",
    "[ 51 ]  banana\n",
    "[ 52 ]  apple\n",
    "[ 53 ]  sandwich\n",
    "[ 54 ]  orange\n",
    "[ 55 ]  broccoli\n",
    "[ 56 ]  carrot\n",
    "[ 57 ]  hot dog\n",
    "[ 58 ]  pizza\n",
    "[ 59 ]  donut\n",
    "[ 60 ]  cake\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de1683-c1c0-4435-b754-1c91aad560d7",
   "metadata": {},
   "source": [
    "Once the dataset-related information is loaded, let's do the same with the network configuration and its file of weights. We are going to give a try to SSD Mobile net, but this is just one of the possibilities supported by OpenCV DNN. [Here you can find a list](https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV) with all the networks that have been tested by the OpenCV community. If you want to try another one, [this file contains](https://github.com/opencv/opencv_extra/blob/master/testdata/dnn/download_models.py) the url to download the models' weights, while [this repository](https://github.com/opencv/opencv_extra/tree/4.x/testdata/dnn) provides the networks' configuration files.\n",
    "\n",
    "SSD comes from Single Shot MultiBox Detector, and it is a popular network presented at the popular European Conference on Computer Vision (ECCV) in 2016<sup>[[1]](#cite1)</sup>. This network consists of two parts:\n",
    "- A backbone (VGG-16 in the image, but in this case MobileNet is employed) used to extract feature maps from the image and a number of extra feature layers to detect objects at different scales, and\n",
    "- a number of convolutional layers that carry out the object detections (shown over the lines with the prefix *Classifier*).\n",
    "\n",
    "The network architecture is depicted in Fig. 5.\n",
    "\n",
    "<center><img src=\"./images/dnn/ssd_network_architecture.PNG\"><br /><i>Figure 5. Architecture of the SSD network [Source SSD paper]. </i></center>$\\\\[5pt]$\n",
    "\n",
    "Let's load this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da2f35-64a7-4056-98ec-d6dcb00afcc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the DNN model\n",
    "model = cv2.dnn.readNet(model='./data/frozen_inference_graph.pb',  \n",
    "                        config='./data/ssd_mobilenet_v2_coco_2018_03_29.pbtxt',framework='TensorFlow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72a7d3-5853-4722-9e77-0861d862dc7b",
   "metadata": {},
   "source": [
    "## 7.4.2 Doing inference and showing results\n",
    "\n",
    "The following code loads an image from file, gets its height and width, and set it as input to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8884a9d-3765-48de-a788-2f9c7e0ccb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image from disk\n",
    "image = cv2.imread('./images/dnn/kitchen_2.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_height, image_width, _ = image.shape\n",
    "# Create blob from image\n",
    "blob = cv2.dnn.blobFromImage(image=image, size=(300, 300), mean=(104, 117, 123), swapRB=False)\n",
    "# Set the blob to the model\n",
    "model.setInput(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe06df-ff1f-4590-97ef-7cced80044bf",
   "metadata": {},
   "source": [
    "Once the information related to the dataset and the network are loaded, as well as an image to be processed, we can perform inference by just calling the ```forward()``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bfa59-7541-4d4c-8018-caf037f329b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the model to carry out the detection\n",
    "output = model.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ac0f4-e419-4998-80db-71501221b22b",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Showing the results</i></b></span>**\n",
    "\n",
    "The network output is a set of predictions consisting of object categories, scores and bounding boxes, concretely:\n",
    "```prediction=[not_used,category,score,bbox_x_ul,bbox_y_ul,bbox_x_br,bbox_y_br]```\n",
    "\n",
    "For example, the next output:\n",
    "\n",
    "```[ 0.         52.          0.9821959   0.3924626   0.60919476  0.85021377  0.9348804 ]```\n",
    "\n",
    "says that it has been detected an apple with a very high score (it ranges from 0 to 1) with a bounding box with the upper left corner at position $[x_{ul},y_{ul}]=[0.3924626 \\times img\\_width,0.609194 \\times img\\_height]$ and with the bottom right corner at position $[x_{br},y_{br}]=[0.850213\\times img\\_width,0.93488\\times img\\_height]$.\n",
    "\n",
    "You are tasked to:\n",
    "- Get the confidence (score) value from the ```detection``` vector, and set a threshold so only detections with a confidence value higher than it are shown (try different values until you get a result similar to the one shown in Fig. 6),\n",
    "- get the boundig boxes positions and sizes as commented before,\n",
    "- show the rectangles representing the bounding boxes, and\n",
    "- build the ```text``` string containing the class names and confidence values as shown in Fig. 6.\n",
    "\n",
    "<center><img src=\"./images/dnn/example_of_results.PNG\"><br /><i>Figure 6. Illustration of an image processed by the network with annotations of the detected objects, their categories and scores. </i></center>$\\\\[5pt]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82947d0-dd0b-4310-90c8-02e892a1d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each of the detection\n",
    "image_copy = image.copy()\n",
    "\n",
    "for detection in output[0, 0, :, :]:   \n",
    "    \n",
    "    # extract the confidence of the detection\n",
    "    confidence = None\n",
    "    \n",
    "    # draw bounding boxes only if the detection confidence is above...\n",
    "    # ... a certain threshold, else skip\n",
    "    if confidence > None:\n",
    "\n",
    "        # get the class id\n",
    "        class_id = detection[1]\n",
    "        # map the class id to the class\n",
    "        class_name = class_names[int(class_id)-1]\n",
    "        color = COLORS[int(class_id)]\n",
    "        # get the bounding box upper left corner coordinates\n",
    "        box_x_ul = None\n",
    "        box_y_ul = None\n",
    "        # get the bounding box bottom right corner coordinates\n",
    "        box_x_br = None\n",
    "        box_y_br = None\n",
    "        # draw a rectangle around each detected object\n",
    "        cv2.rectangle(image_copy, (int(None), int(None)), (int(None), int(None)), color, thickness=2)\n",
    "        # put the FPS text on top of the frame\n",
    "        text = None\n",
    "        cv2.putText(image_copy, text, (int(box_x_ul+2), int(box_y_ul + 30)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "# Show image_copy with the detected objects\n",
    "plt.imshow(image_copy);\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29e8ef-4802-4e05-86cd-443dd4fc8cb5",
   "metadata": {},
   "source": [
    "With this type of models onboard of a mobile robot, it would be able to detect the objects in its sourroundings and use this information during its operation. Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0719d67-e4bb-4f61-b7a5-cc80d5175b73",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Cool! You have experienced the trendy Deep Learning world a bit.\n",
    "\n",
    "In this notebook you have learned:\n",
    "\n",
    "- how look for different DL models that has been tested by the OpenCV community,\n",
    "\n",
    "- how to load information related to the dataset at hand, as well as how to load the network weights (model) and configuration,\n",
    "\n",
    "- how to do inference and visualize the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e830a5-665f-4628-b072-27fdc86cc0d2",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>Optional</i></b></span>**\n",
    "\n",
    "Now that you know how to use OpenCV DNN you can challenge the model with other images or, even better, try another object detection network or a model targeted at a different task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb62a36-73f1-4d12-85ed-94c43ad6031a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a name=\"myfootnote1\">[1]</a>: LIU, Wei, et al. [Ssd: Single shot multibox detector](https://arxiv.org/abs/1512.02325). In European Conference on Computer Vision (ECCV). Springer, Cham, 2016. p. 21-37."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
