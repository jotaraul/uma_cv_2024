{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.2 Epipolar geometry\n",
    "\n",
    "Epipolar geometry is the **set of geometric constraints between two views of a scene**. When two cameras capture a 3D scene from two different positions, there are a number of geometric relations between the 3D points and their projections onto the 2D images that lead to constraints between the image points. These relations are derived based on the assumption that the cameras can be approximated by the pinhole camera model (e.g. their images are undistorted). $\\\\[10pt]$\n",
    "\n",
    "In this notebook, we will go deeper into epipolar geometry, learning:\n",
    "\n",
    "- Basic concepts of epipolar geometry (<a href=\"#1021\">section 10.2.1</a>)\n",
    "- Stereo rectification (<a href=\"#1022\">section 10.2.2</a>)\n",
    "- Essential matrix (<a href=\"#1023\">section 10.2.3</a>), and\n",
    "- Fundamental matrix (<a href=\"#1024\">section 10.2.4</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem context - Stereo rectification\n",
    "\n",
    "In practice, as we depicted in the previous notebook, accurate triangulation is not fully feasible as **stereo systems does not fulfill the ideal configuration**, but the good news is that any two images taken with any configuration (as long as there is overlap between the images, of course) can be transformed so that they comply with an ideal stereo camera configuration. This is called **stereo rectification** or **image rectification**. Here, we apply homographies to the left and right images in order to force this ideal configuration.$\\\\[10pt]$\n",
    "\n",
    "<center><img src=\"./images/rectification.png\" width=\"600\" /></center>\n",
    "\n",
    "Image rectification is based in epipolar geometry so, let's start to learn how it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.1 Concepts of epipolar geometry <span id=\"1021\"></span>\n",
    "\n",
    "Before starting to explain how epipolar geometry works, we have to understand some basic concepts: the **epipolar plane**, the **epipolar lines** and the **epipoles**.\n",
    "\n",
    "First of all, we have the **epipolar plane**. This plane is formed by 3 points: the left and right optical centers ($C_l,\\ C_r$), and a 3D point in the world ($\\mathbf{X}$). \n",
    "\n",
    "<center><img src=\"./images/epipolar1.png\" width=\"600\" /></center>\n",
    "\n",
    "The interesting thing about the epipolar plane, is that it **intersects the images at the conjugate epipolar lines** of the stereo vision system. The 2D projections $(x_l, x_r)$ of the 3D point $\\mathbf{X}$ are contained also in the epipolar plane and **lie on their corresponding epipolar lines**. As can be seen in the image, these epipolar lines are the **intersection of the epipolar plane and the image planes**.\n",
    "\n",
    "As you might realize, it would be interesting to compute these epipolar lines because they will help us to check **whether a pair of corresponding points is correct or not** (in order to find the matching point in the other image, you don't need to search the whole image, just to search along the epipolar line!). This is called the **epipolar constraint** useful to transform a search in 2D into a 1D problem. Interestingly, this is valid for all the pixels in the images, that is, all the pixels will have their corresponding epipolar line (or **epiline**, for short) in the other image. And even more interestingly, the direction of the epilines **do not depend on the scene** but only on the camera configuration! That is, if you have a rigid stereo camera pair, the epilines will not change regardless the scene you are viewing! So:\n",
    "\n",
    "> Given a two-camera configuration (represented by $R,t$), for each 3D point there is a **unique epipolar plane** and, consequently, a **unique pair of epipolar lines**.\n",
    "\n",
    "On the other hand, maybe you have noticed that all the epipolar lines **intersect in one single point at each image**, this point is called the **epipole**, and represents the projection of the optical center of each camera on the other image plane (note that it may be outside the boundaries of the image!). All the epilines pass through its epipole. So, one way to determine the position of the epipole is to compute a set of epilines and to find their intersection point.\n",
    "\n",
    "<center><img src=\"./images/epipoles.png\" width=\"450\" /></center>\n",
    "\n",
    "There are some special cases where one or both epipoles **are placed at the infinity** (if the image plane is parallel to the line $C_l-C_r$), which implies that we do not know their location but their *direction*:\n",
    "\n",
    "<center><table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images/epipolar3.png\" /></td>\n",
    "        <td><img src=\"./images/epipoles_inf.png\" /></td>\n",
    "    </tr>\n",
    "</table></center>\n",
    "\n",
    "This means that, in this situation, the epilines are **horizontal and parallel**, which is quite handy as we depicted in the previous notebook. Notice that, in such case, the third coordinate of the homogeneous representation of the **epipole position is zero**. In these cases, we can compute the direction in which the epipole can be found:\n",
    "\n",
    "<center><img src=\"./images/epipolar4.png\" width=\"800\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are the epipoles?\n",
    "\n",
    "First of all, let's **find the coordinates of an optical center** $\\mathbf{C}$ in the `WORLD` reference frame, that is $\\mathbf{C}^W$. (*Remember that the superscript in a point/vector indicates the reference frame*). Obviously, in the local reference system of the same camera, its coordinates are equal to zero (the optical center is the origin of coordinates!): $\\mathbf{C}^C = \\mathbf{0}$): $\\\\[10pt]$\n",
    "\n",
    "<img src=\"./images/epipolar5.png\" width=\"500\"/>\n",
    "\n",
    "$$\\begin{cases} \\mathbf{C}^C = \\mathbf{R}_W^C \\mathbf{C}^W + \\mathbf{t}_W^C \\\\[10pt] \\mathbf{C}^C = \\begin{bmatrix}0\\\\0\\\\0 \\end{bmatrix} = \\bf{0}\\end{cases} \\longrightarrow \\mathbf{0} = \\mathbf{R}_W^C \\mathbf{C}^W + \\mathbf{t}_W^C \\rightarrow \\mathbf{C}^W = -(\\mathbf{R}_W^C)^\\texttt{T}\\mathbf{t}_W^C\\\\[20pt]$$\n",
    "\n",
    "For the left camera, and since the `WORLD` frame is assumed to be placed at the optical center of the left camera, we have that: $\\mathbf{R}_W^{C_l} = I, \\; \\mathbf{t}_W^{C_l} = \\mathbf{0}$, that is: $\\mathbf{C}_l^W = \\mathbf{0}$. $\\\\[5pt]$\n",
    "\n",
    "Analogously, the right optical center has these coordinates: $C_r^W = -(\\mathbf{R}_W^{C_r})^\\texttt{T}\\mathbf{t}_W^{C_r} = -\\mathbf{R}^\\texttt{T}\\mathbf{t}\\hspace{2cm}$ $\\\\[10pt]$\n",
    "\n",
    "That is, the left optical center is placed at the origin of coordinates while the right one has an arbitrary rotation and a translation with respect to it, which, for simplicity we will call $\\mathbf{R}$ and $\\mathbf{t}$.\n",
    "\n",
    "Now, let's find the left and right epipoles $(e_l,e_r)$ by determining the projection of the optical centers on the other image plane. Remember the general equation for image projection in previous notebooks. *Note: in these equations, the apostrophe in $\\tilde{e}'$ means the coordinates are expressed in pixels, while the tilde $\\sim$ means it is in homogeneous coordinates.*:$\\\\[5pt]$\n",
    "\n",
    "$$e_l:  \\lambda\\tilde{e}'_l = \\mathbf{P}_l \\tilde{C}_r^W = \\mathbf{K}_l [\\mathbf{I}|\\mathbf{0}]\n",
    "\\begin{bmatrix}-\\mathbf{R}^\\texttt{T} \\mathbf{t}\\\\1 \\end{bmatrix} = -\\mathbf{K}_l \\mathbf{R}^\\texttt{T} \\mathbf{t}\\\\\n",
    "  e_r:  \\lambda\\tilde{e}'_r = \\mathbf{P}_r \\tilde{C}_l^W = \\mathbf{K}_r [\\mathbf{R}|\\mathbf{t}]\n",
    "\\begin{bmatrix}\\mathbf{0}\\\\1 \\end{bmatrix} = \\mathbf{K}_r \\mathbf{t} \\hspace{1.4cm}\n",
    "\\\\[5pt]$$\n",
    "\n",
    "where $\\mathbf{K}_l$ and $\\mathbf{K}_r$ are the calibration matrices of the left and right camera, respectively.\n",
    "\n",
    "Finally, the epipoles in the **ideal configuration** for triangulation (cameras with parallel optical axis, separated by a baseline, and with the same intrinsic parameters):$\\\\[5pt]$\n",
    "\n",
    "$$\\mathbf{R} = \\mathbf{I},\\quad \\mathbf{t} = [-b\\ 0 \\ 0]^\\texttt{T}, \\quad \\mathbf{K} = \\mathbf{K}_l = \\mathbf{K}_r\\\\[3pt]\n",
    "\\lambda\\tilde{e}'_l = -\\mathbf{K}\\mathbf{t} = [bk_xf \\quad 0 \\quad 0]^\\texttt{T}\\\\\n",
    "\\lambda\\tilde{e}'_r = \\mathbf{K}\\mathbf{t} = [-bk_xf \\quad 0 \\quad 0]^\\texttt{T}$$\n",
    "\n",
    "As we said before, **they are points at infinity in the direction of the $X$ axis!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.2 Stereo rectification <span id=\"1022\"></span>\n",
    "\n",
    "In practice, our camera configuration will not be ideal (i.e. $\\mathbf{R} \\ne \\mathbf{I} \\wedge \\mathbf{t} \\ne [-b\\ 0 \\ 0]^\\texttt{T} \\wedge \\mathbf{K}_l \\ne \\mathbf{K}_r$), and this involves a difficulty to **find the correspondences and to perform stereo triangulation**:\n",
    "\n",
    "<center><img src=\"./images/non-rectified.png\" width=\"400\" /></center>\n",
    "\n",
    "As you can see in the images, **the epipolar lines are not horizontal** as in the ideal configuration, but this can be solved. The process of **stereo rectification** projects the images on a common plane so that the epipolar lines become horizontal in both images and at the same height, that is, transform them as if they had been captured in an ideal configuration, taking the **epipoles to infinity**.\n",
    "\n",
    "For that, we can compute a **homography for each camera** that transforms the images to such ideal configuration:\n",
    "\n",
    "<center><img src=\"./images/rectified.png\" width=\"400\" /></center\n",
    "\n",
    "#### <font color=\"orange\">OpenCV hint</font>\n",
    "\n",
    "OpenCV defines a method for image rectification called ([cv2.stereoRectify](https://docs.opencv.org/4.3.0/d9/d0c/group__calib3d.html#ga617b1685d4059c6040827800e72ad2b6)), which computes the rotation matrices for each camera that (virtually) make both camera image planes the same plane. This method asks for the matrices $\\mathbf{R}_r^l$ and $\\mathbf{t}_r^l$, as well as the calibration matrices and then applies the camera model to compute such rotation matrices. As we don't have a stereo system that have a fixed relative pose, we cannot obtain those matrices (they could be obtained through [stereo calibration](https://docs.opencv.org/4.3.0/d9/d0c/group__calib3d.html#ga91018d80e2a93ade37539f01e6f07de5)).\n",
    "\n",
    "Fortunately, there exists the method [cv2.stereoRectifyUncalibrated](https://docs.opencv.org/4.3.0/d9/d0c/group__calib3d.html#gaadc5b14471ddc004939471339294f052), which returns the **rectification homographies** for both images given a set of matching points and the **fundamental matrix** (explained later). This method is used when **you don't know the relative pose of the cameras** that captured the pictures (e.g. two images taken with the same camera but in different positions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10.2.3 The Essential Matrix <span id=\"1023\"></span>\n",
    "\n",
    "At the begining of the notebook, we stated that every point in an image will have a corresponding epipolar line in the other image, which was given by the intrinsic epipolar geometry between the views. **The Essential Matrix (E)** is a $3 \\times 3$ matrix that encodes such epipolar geometry **when the cameras are calibrated**. So, given a point in one image, multiplying it by the essential matrix will tell us the epipolar line in the second view:\n",
    "\n",
    "$$l_l = \\mathbf{E}\\tilde{x_r}$$\n",
    "\n",
    "<center><img src=\"./images/essential.png\" width=\"600\" /></center>\n",
    "\n",
    "where $l_l$ is the epiline in the left image corresponding to the point $x_r$ in the right one.\n",
    "\n",
    "But, **how can we compute the essential matrix?**\n",
    "\n",
    "<center><img src=\"./images/essential2.png\" width=\"800\" /></center>\n",
    "\n",
    "As can be seen in the image, vectors $\\mathbf{X}_l$ (the coordinates of a 3D point in the left reference system), $\\mathbf{t}_r^l$ (the coordinates of the right optical center in the left reference system) and $\\mathbf{RX}_r$ (the coordinates of a 3D point in the right reference system) **are coplanar**, so, by definition, their triple product is zero: $\\\\[5pt]$\n",
    "\n",
    "$$\\mathbf{X}_l^\\texttt{T} \\cdot [\\mathbf{t}_r^l \\times (\\mathbf{RX}_r)] = 0 $$\n",
    "\n",
    "This can be simplified by using the coordinate vectors of the projected points on the image plane: $\\text{X} = \\lambda \\tilde{x}$, which are also coplanar.\n",
    "\n",
    "$$\n",
    "  \\mathbf{ \\tilde{x} }_l^\\texttt{T} [\\mathbf{t}_r^l \\times (\\mathbf{R\\tilde{x}}_r)] =\n",
    "  \\mathbf{ \\tilde{x} }_l^\\texttt{T} \\underbrace{([\\mathbf{t}_r^l]_\\times \\mathbf{R})}_{\\mathbf{E}}\\tilde{x}_r = 0 \\Longrightarrow \\\\\n",
    "  \\boxed{\\Large{\\mathbf{\\tilde{x}}_l^\\texttt{T} \\mathbf{E} \\mathbf{\\tilde{x}}_r = 0}} \\\\\n",
    "$$\n",
    "\n",
    "The problem of this matrix is that we are working in sensor coordinates (meters), so **the cameras need to be calibrated** in order to transform such coordinates to pixels, that is, $\\mathbf{K}$ must be known, so that (assuming identical intrinsic parameters):\n",
    "\n",
    "$$\\mathbf{\\tilde{x}}_r' = \\mathbf{K}\\mathbf{\\tilde{x}}_r \\\\ \\mathbf{\\tilde{x}}_l' = \\mathbf{K}\\mathbf{\\tilde{x}}_l$$\n",
    "\n",
    "Given this limitation, the use of the **Fundamental Matrix** seems to be more appealing, since it can work with **uncalibrated cameras**. Let's discover it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2.4 The Fundamental Matrix <span id=\"1024\"></span>\n",
    "\n",
    "The **Fundamental Matrix (F)** is similar to the Essential Matrix, but it works in the **image plane instead of with sensor coordinates**, but they have similar properties and expressions:\n",
    "\n",
    "$$\\boxed{\\Large{\\mathbf{\\tilde{x}}'^\\texttt{T}_l \\mathbf{F} \\mathbf{\\tilde{x}}'_r = 0}}$$\n",
    "\n",
    "We can obtain the fundamental matrix from the essential one through:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    \\mathbf{\\tilde{x}}^\\texttt{T}_l \\mathbf{E} \\mathbf{\\tilde{x}}_r \n",
    "    & = & \\left( \\mathbf{K}^{-1} \\mathbf{\\tilde{x}}'_l \\right)^\\texttt{T}  [\\mathbf{t}]_\\times \\mathbf{R} \\left(\\mathbf{K}^{-1} \\mathbf{\\tilde{x}}'_r\\right) \\\\\n",
    "    & = & \\left( \\mathbf{\\tilde{x}}'^\\texttt{T}_l \\mathbf{K}^{\\texttt{-T}} \\right) [\\mathbf{t}]_\\times \\mathbf{R} \\left(\\mathbf{K}^{-1} \\mathbf{\\tilde{x}}'_r\\right) \\\\ \n",
    "    & = & \\mathbf{\\tilde{x}}'^\\texttt{T}_l \\left(\\mathbf{K}^{\\texttt{-T}} [\\mathbf{t}]_\\times \\mathbf{R} \\mathbf{K}^{-1} \\right) \\mathbf{\\tilde{x}}'_r \\\\ \n",
    "    & = & \\mathbf{\\tilde{x}}'^\\texttt{T}_l \\mathbf{F} \\mathbf{\\tilde{x}}'_r = 0\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "So, we have that\n",
    "$$\\boxed{\\Large{\\mathbf{F} = \\mathbf{K}^\\texttt{-T} [\\mathbf{t}]_\\times \\mathbf{R} \\mathbf{K}^{-1} = \\mathbf{K}^\\texttt{-T} \\mathbf{E} \\mathbf{K}^{-1}}}$$\n",
    "\n",
    "The good thing about working in the image plane, is that we can obtain the matrix $\\mathbf{F}$ from 8 pairs of points in both images, without needing any knowledge about the calibration matrix $\\mathbf{K}$).\n",
    "\n",
    "The **Fundamental matrix is the cornerstone of the epipolar geometry**, as it relates points and their corresponding epipolar lines in these ways:\n",
    "\n",
    "### F relating two corresponding points:\n",
    "We can check if a correspondence of points is valid (points are contained in corresponding epipolar lines):\n",
    "\n",
    "$$\\mathbf{\\tilde{x}}'^\\texttt{T}_l \\mathbf{F} \\mathbf{\\tilde{x}}'_r = 0$$\n",
    "\n",
    "### F relating points with their epipolar lines:\n",
    "It's also possible to find the corresponding epipolar line of a point:\n",
    "\n",
    "$$\\mathbf{l}'_l = \\mathbf{F} \\mathbf{\\tilde{x}}'_r \\quad \\mathbf{l}'_r = \\mathbf{F}^\\texttt{T} \\mathbf{\\tilde{x}}'_l$$\n",
    "\n",
    "### F used to compute the epipoles:\n",
    "As the epipole is the intersection point of all epipolar lines, the epipole is the null space of the Fundamental matrix:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    \\mathbf{\\tilde{x}}'^\\texttt{T}_l \\mathbf{F} \\mathbf{\\tilde{e}}'_r & = & 0,  \\quad \\forall \\mathbf{\\tilde{x}}'_l\\\\\n",
    "    \\mathbf{F} \\mathbf{\\tilde{e}}'_r & = & \\mathbf{F}^\\texttt{T} \\mathbf{\\tilde{e}}'_l = 0\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "### BONUS: F used to feel the rhythm:\n",
    "The Fundamental matrix is so popular in computer vision that it even has a [funny song](https://www.youtube.com/watch?v=DgGV3l82NTk)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "\n",
    "images_path = './images/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.drawlines import drawlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 1: Retrieving the Fundamental matrix</i></b></span>**\n",
    "\n",
    "\n",
    "In this task, we are going to rectify two images using the above-mentioned method [`cv2.stereoRectifyUncalibrated()`](https://docs.opencv.org/4.3.0/d9/d0c/group__calib3d.html#gaadc5b14471ddc004939471339294f052). For that, you has to take two images of the same scene from different positions, imitating a stereo system. *Note: you can use images `iml3.png` (left) and `imr3.png` (right) provided to you with this notebook if you can't use your own images*.\n",
    "\n",
    "The first step for image rectification is **finding the Fundamental matrix**, and for this we need a set of matching points! Use your favourite keypoint detector + descriptor in order to get proper a set of matches (we recommend **ORB for big images**).\n",
    "\n",
    "**What to do?** Compute the Fundamental matrix that relates the epipolar geometry between your images and show it. For that you have to:\n",
    "\n",
    "1. Load the images and convert them to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT 1\n",
    "# Write your code here!\n",
    "# Read left image\n",
    "im_l = cv2.imread(images_path + None)  # queryimage\n",
    "im_l = cv2.cvtColor(im_l, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Read right image\n",
    "im_r = cv2.imread(images_path + None)  # trainimage\n",
    "im_r = cv2.cvtColor(im_r, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Get gray images\n",
    "gray_l = cv2.cvtColor(im_l,cv2.COLOR_RGB2GRAY)\n",
    "gray_r = cv2.cvtColor(im_r,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "print(\"Image sizes:\",gray_l.shape,\"and\",gray_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Detect keypoints in both images and find correspondences (do matching).\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.1 Option 1, **using ORB.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOTHER OPTION: USE ORB KEYPOINTS\n",
    "# Detect the ORB keypoints using the OpenCV method\n",
    "# -- create the ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# -- detect ORB keypoints \n",
    "kp_l = orb.detect(None,None) # Leave the second None as it is\n",
    "kp_r = orb.detect(None,None) # Leave the second None as it is\n",
    "\n",
    "# -- compute the descriptors with ORB\n",
    "kp_l, des_l = orb.compute(None, None)\n",
    "kp_r, des_r = orb.compute(None, None)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck=True).match(des_l,des_r)\n",
    "\n",
    "print(\"Number of ORB matches:\",len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2 Option 2. Using **SURF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # detect keypoints using the SURF method\n",
    "# surf = cv2.xfeatures2d.SURF_create(2000)\n",
    "   \n",
    "# kp_l, des_l = surf.detectAndCompute(gray_l,None)\n",
    "# kp_r, des_r = surf.detectAndCompute(gray_r,None)\n",
    "   \n",
    "# # Call knnMatch\n",
    "# pre_matches = cv2.BFMatcher().knnMatch(des_l,des_r, k=2)\n",
    "\n",
    "# # Filter non-robust matches\n",
    "# matches = [m for m,n in pre_matches if m.distance < 0.60*n.distance]\n",
    "\n",
    "# print(\"Number of SURF matches:\",len(matches),\"out of\",len(pre_matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Compute the Fundamental matrix**. OpenCV already implements a method for computing the Fundamental matrix ([`cv2.findFundamentalMat()`](https://docs.opencv.org/4.2.0/d9/d0c/group__calib3d.html#gae420abc34eaa03d0c6a67359609d8429)) from two lists of matching points (unfortunately using their coordinates, not `cv2.keyPoint()` objects). Apart from the Fundamental matrix, this method also returns a vector `mask`, which states the points used for the matrix calculation. Play with the `ransacReprojThreshold` parameter in such function (take a look at it in the OpenCV documentation) and check how many matches are considered inliers depending on it. The expected output is obtained with `ransacReprojThreshold=0.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fundamental matrix\n",
    "\n",
    "# Create lists of corresponding keypoints\n",
    "pts_l = []\n",
    "pts_r = []\n",
    "for match in matches:\n",
    "    pts_l.append(kp_l[match.queryIdx].pt)\n",
    "    pts_r.append(kp_r[match.trainIdx].pt)\n",
    "    \n",
    "# Cast to integers\n",
    "pts_l = np.int32(pts_l)\n",
    "pts_r = np.int32(pts_r)\n",
    "\n",
    "# F, mask = cv2.findFundamentalMat(pts_l,pts_r,cv2.FM_LMEDS)\n",
    "F, mask = cv2.findFundamentalMat(None, None, cv2.FM_RANSAC, None, 0.99)\n",
    "\n",
    "# keep only the inliers (matches)\n",
    "matches_in = [matches[i] for i in range(len(mask)) if mask[i]==1]\n",
    "pts_l = [kp_l[m.queryIdx].pt for m in matches_in]\n",
    "pts_r = [kp_r[m.trainIdx].pt for m in matches_in]\n",
    "\n",
    "# Cast to integers\n",
    "pts_l = np.int32(None)\n",
    "pts_r = np.int32(None)\n",
    "\n",
    "print(\"Number of inliers =\", len(matches_in), \"out of\", len(matches))\n",
    "print(\"Fundamental matrix =\")\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if your results are correct**:\n",
    "\n",
    "<font color='blue'>**Expected output:**  </font>\n",
    "\n",
    "    Number of inliers = 173 out of 277\n",
    "    Fundamental matrix =\n",
    "    [[-3.49654807e-07 -7.20777510e-05  1.99345790e-02]\n",
    "     [ 6.49820354e-05  2.66731011e-05  4.27861666e-01]\n",
    "     [-1.99589277e-02 -4.36546231e-01  1.00000000e+00]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. And finally, show the used matches!\n",
    "\n",
    "**You should get something like:**\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/expected_output_assignment1.png\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot matches\n",
    "matches_image = np.copy(im_l)\n",
    "matches_image = cv2.drawMatches(None, None, None, None, None, \n",
    "                                matches_image, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(matches_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 2: Rectifying images!</i></b></span>**\n",
    "\n",
    "\n",
    "Now, we can **compute the homographies** needed to rectify the images with the method [`cv2.stereoRectifyUncalibrated()`](https://docs.opencv.org/4.3.0/d9/d0c/group__calib3d.html#gaadc5b14471ddc004939471339294f052), which takes as input\n",
    "- **the lists of matching points** used in the previous method, \n",
    "- **the Fundamental matrix**, \n",
    "- and **the shape of the images**.\n",
    "\n",
    "and returns the computed homographies. Once you have them, you already know how to apply an homography to an image, right? Well, then do it!\n",
    "\n",
    "**What to do?** Compute the homographies, use them to rectify your images and show them.\n",
    "\n",
    "**You should get something like:**\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/expected_output_assignment2.png\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "\n",
    "# ASSIGNMENT 2\n",
    "# Write your code here!\n",
    "# Obtain homographies\n",
    "ret, H_l, H_r = cv2.stereoRectifyUncalibrated(None,None, None, gray_r.shape)\n",
    "\n",
    "# Apply homographies\n",
    "rectified_l = cv2.warpPerspective(None, None, (im_l.shape[1],im_l.shape[0]))\n",
    "rectified_r = cv2.warpPerspective(None, None, (im_l.shape[1],im_l.shape[0]))\n",
    "\n",
    "# Show rectified images\n",
    "plt.subplot(221)\n",
    "plt.title(\"Original left image\")\n",
    "plt.imshow(None)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title(\"Original right image\")\n",
    "plt.imshow(None)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title(\"Rectified left image\")\n",
    "plt.imshow(None)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title(\"Rectified right image\")\n",
    "plt.imshow(None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 3: Checking the rectification</i></b></span>**\n",
    "\n",
    "Now, plot the images side-by-side (i.e., concatenate them with [`np.concatenate()`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html)) and check that corresponding points fall in the same row! For that, just pick some points in the left image and draw a line on the same row in the right image.\n",
    "\n",
    "**You should get something like:**\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/expected_output_assignment3.png\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT 3\n",
    "# Write your code here!\n",
    "total = np.concatenate((None, None), axis=1)\n",
    "\n",
    "_,offset = rectified_r.shape[:2]\n",
    "size = 1\n",
    "cont = 0\n",
    "for p in pts_l:\n",
    "    # for each point\n",
    "   \n",
    "    # apply left homography\n",
    "    hom_p = np.vstack([p[0],p[1],1])  # homogeneous point\n",
    "    tran_hom_p = H_l @ hom_p\n",
    "    \n",
    "    x = np.int32(None[0]/None[2])\n",
    "    y = np.int32(None[1]/None[2])\n",
    "    \n",
    "    if y > 0:\n",
    "        cont+=1\n",
    "        color = tuple(np.random.randint(0,255,3).tolist())\n",
    "        cv2.circle(total,(int(x),int(y)), 2*size, color, -1)\n",
    "        cv2.line(total, (int(offset),int(y)), (int(2*offset),int(y)), color, size)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(total)\n",
    "print(\"Number of points:\", cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print homographies values\n",
    "print(\"H_l = \")\n",
    "print(H_l)\n",
    "\n",
    "print(\"H_r = \")\n",
    "print(H_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if your results are correct**:\n",
    "\n",
    "<font color='blue'>**Expected output:**  </font>\n",
    "\n",
    "    H_l = \n",
    "    [[-4.48201778e-01  2.02246376e-02  8.79537497e+00]\n",
    "     [-1.94501778e-02 -4.36633942e-01  4.44143313e+00]\n",
    "     [-6.31237657e-05 -2.52599836e-05 -4.15827289e-01]]\n",
    "    H_r = \n",
    "    [[ 1.02945316e+00 -9.58077348e-03 -3.24241377e+00]\n",
    "     [ 4.48323907e-02  9.99626066e-01 -8.03314514e+00]\n",
    "     [ 1.62963903e-04 -1.51665010e-06  9.70834163e-01]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<span style=\"color:green\"><b><i>ASSIGNMENT 4: Computing the epipolar lines</i></b></span>**\n",
    "\n",
    "\n",
    "Having the Fundamental matrix, **we can also calculate the epipolar lines** that correspond to any point, **even if the images are not rectified**. Remember that the Fundamental matrix encompasses the intrinsic geometric information between two views, regardless the scene!. The OpenCV's method [cv2.computeCorrespondEpilines](https://docs.opencv.org/4.2.0/d9/d0c/group__calib3d.html#ga19e3401c94c44b47c229be6e51d158b7) facilitates this, having as inputs:\n",
    "\n",
    "- `points`: set of $N$ input points of which corresponding epipolar lines will be computed\n",
    "- `whichImage`: index of the image (1 for left or 2 for right) that contains the points\n",
    "- `F`: the Fundamental matrix\n",
    "\n",
    "It returns the **coefficients of the epipolar lines** in a ($N \\times 1 \\times 3$) matrix.\n",
    "\n",
    "To show the epipolar lines, we have provided to you a method called `drawlines` (in utils) that plots the epilines in one image, and their corresponding keypoints (with the same color) in the other image, its inputs are:\n",
    "\n",
    "- `im_points`: input image in which the points are going to be drawn\n",
    "- `pts`: array containing the points' coordinates ($N \\times 2$) of `im_points`\n",
    "- `im_lines`: input image in which the epipolar lines are going to be drawn\n",
    "- `lines`: coefficients of the epipolar lines drawn in `im_lines` \n",
    "- `size`: size of the elements drawn (default 5). Change this if you cannot see the lines and points or they are too big.\n",
    "\n",
    "It returns two images, `im_lines` (with the lines superimposed) and `im_points` (where the points have been drawn).\n",
    "\n",
    "**What to do?** Calculate the **epipolar lines of the keypoints previously detected in the right image**. Then, **draw the epipolar lines in the left image** and the points in the right one.\n",
    "\n",
    "Finally, display both images.\n",
    "\n",
    "**You should get something like:**\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/expected_output_assignment4.png\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT 4\n",
    "# Write your code here!\n",
    "\n",
    "# Find epilines corresponding to points in right image\n",
    "lines_l = cv2.computeCorrespondEpilines(pts_r.reshape(-1,1,2), 2, None)\n",
    "lines_l = lines_l.reshape(-1,3)\n",
    "\n",
    "# Draw epipolar lines\n",
    "epipolar_l, epipolar_r = drawlines(None,None,None,None,2)\n",
    "\n",
    "# Print images\n",
    "plt.subplot(121)\n",
    "plt.title('Left image with epipolar lines')\n",
    "plt.imshow(None)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Right image with keypoints')\n",
    "plt.imshow(None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Brilliant! Note that if we had a fixed stereo system, **the Fundamental matrix will be constant for any pair of images**. Anyway, once rectification have been applied, triangulation is more precise, and so would be the depth computation!\n",
    "\n",
    "In this notebook, we have learnt:\n",
    "\n",
    "- the fundamentals of epipolar geometry\n",
    "- how to perform stereo rectification for uncalibrated and calibrated cameras\n",
    "- how to find the Fundamental matrix from a pair of images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
